[2021-12-12 06:49:11,248] [WARNING] [runner.py:122:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2021-12-12 06:49:12,269] [INFO] [runner.py:360:main] cmd = /opt/conda/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=24802 pretrain_glm.py --block-lm --task-mask --bert-prob 0.5 --gap-sentence-prob 0.3 --avg-block-length 3 --gpt-min-ratio 0.25 --block-mask-prob 0.1 --short-seq-prob 0.02 --experiment-name blocklm-10b --model-parallel-size 1 --num-layers 48 --hidden-size 4096 --num-attention-heads 64 --seq-length 513 --max-sequence-length 1025 --save /dataset/fd5061f6/english_data/checkpoints --load /dataset/fd5061f6/english_data/checkpoints/blocklm-10b-512 --log-interval 50 --eval-interval 1000 --save-interval 2000 --train-iters 250000 --train-data wikipedia_ssm --resume-dataloader --filter-english --loader-scatter 32 --tokenizer-type GPT2BPETokenizer --split 949,50,1 --distributed-backend nccl --lr-decay-style cosine --lr-decay-ratio 0.1 --lr-decay-iters 175000 --warmup 0.04 --checkpoint-activations --fp16 --deepspeed --deepspeed_config /dataset/fd5061f6/liuxiao/BlockLM-ssm/config/config_block_10B.json
[2021-12-12 06:49:12,917] [INFO] [launch.py:73:main] 0 NCCL_IB_DISABLE 0
[2021-12-12 06:49:12,917] [INFO] [launch.py:73:main] 0 NCCL_DEBUG info
[2021-12-12 06:49:12,917] [INFO] [launch.py:73:main] 0 NCCL_NET_GDR_LEVEL 2
[2021-12-12 06:49:12,917] [INFO] [launch.py:80:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2021-12-12 06:49:12,917] [INFO] [launch.py:86:main] nnodes=1, num_local_procs=8, node_rank=0
[2021-12-12 06:49:12,918] [INFO] [launch.py:101:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2021-12-12 06:49:12,918] [INFO] [launch.py:102:main] dist_world_size=8
[2021-12-12 06:49:12,918] [INFO] [launch.py:104:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
using world size: 8 and model-parallel size: 1 
> initializing model parallel with size 1
Traceback (most recent call last):
Traceback (most recent call last):
  File "pretrain_glm.py", line 543, in <module>
  File "pretrain_glm.py", line 543, in <module>
    main()
  File "pretrain_glm.py", line 450, in main
    main()
  File "pretrain_glm.py", line 450, in main
    tokenizer = make_tokenizer(args)
      File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
tokenizer = make_tokenizer(args)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
        tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)

  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
    get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
    get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
    text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
    text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
Traceback (most recent call last):
  File "pretrain_glm.py", line 543, in <module>
    vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)
  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
    vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)
  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
    a = os.fspath(a)
    TypeErrora = os.fspath(a): 
expected str, bytes or os.PathLike object, not NoneType
TypeError: expected str, bytes or os.PathLike object, not NoneType
    main()
  File "pretrain_glm.py", line 450, in main
[2021-12-12 06:49:16,767] [INFO] [checkpointing.py:734:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
    tokenizer = make_tokenizer(args)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
[2021-12-12 06:49:16,767] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
Traceback (most recent call last):
  File "pretrain_glm.py", line 543, in <module>
    tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
    main()
  File "pretrain_glm.py", line 450, in main
    tokenizer = make_tokenizer(args)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
    get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
    text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
Traceback (most recent call last):
  File "pretrain_glm.py", line 543, in <module>
Traceback (most recent call last):
  File "pretrain_glm.py", line 543, in <module>
    tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
    main()
      File "pretrain_glm.py", line 450, in main
main()
  File "pretrain_glm.py", line 450, in main
Traceback (most recent call last):
  File "pretrain_glm.py", line 543, in <module>
    vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)
  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
    get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
        tokenizer = make_tokenizer(args)tokenizer = make_tokenizer(args)

      File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
a = os.fspath(a)  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer

TypeError: expected str, bytes or os.PathLike object, not NoneType
    main()
  File "pretrain_glm.py", line 450, in main
    tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
    tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
Traceback (most recent call last):
  File "pretrain_glm.py", line 543, in <module>
    tokenizer = make_tokenizer(args)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
    text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
    main()
          File "pretrain_glm.py", line 450, in main
get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)

  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
    tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
    tokenizer = make_tokenizer(args)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
    text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
        get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)

  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
    vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)
  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
    vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)
  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
    text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType
    get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
    vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)
  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType
    text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
    vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)
  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
    text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType
    vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)
  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType
Killing subprocess 332693
Killing subprocess 332694
Killing subprocess 332695
Killing subprocess 332696
Killing subprocess 332697
Killing subprocess 332698
Killing subprocess 332699
Killing subprocess 332700
Traceback (most recent call last):
  File "/opt/conda/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/launcher/launch.py", line 171, in <module>
    main()
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/launcher/launch.py", line 161, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/launcher/launch.py", line 139, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/bin/python', '-u', 'pretrain_glm.py', '--local_rank=7', '--block-lm', '--task-mask', '--bert-prob', '0.5', '--gap-sentence-prob', '0.3', '--avg-block-length', '3', '--gpt-min-ratio', '0.25', '--block-mask-prob', '0.1', '--short-seq-prob', '0.02', '--experiment-name', 'blocklm-10b', '--model-parallel-size', '1', '--num-layers', '48', '--hidden-size', '4096', '--num-attention-heads', '64', '--seq-length', '513', '--max-sequence-length', '1025', '--save', '/dataset/fd5061f6/english_data/checkpoints', '--load', '/dataset/fd5061f6/english_data/checkpoints/blocklm-10b-512', '--log-interval', '50', '--eval-interval', '1000', '--save-interval', '2000', '--train-iters', '250000', '--train-data', 'wikipedia_ssm', '--resume-dataloader', '--filter-english', '--loader-scatter', '32', '--tokenizer-type', 'GPT2BPETokenizer', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr-decay-style', 'cosine', '--lr-decay-ratio', '0.1', '--lr-decay-iters', '175000', '--warmup', '0.04', '--checkpoint-activations', '--fp16', '--deepspeed', '--deepspeed_config', '/dataset/fd5061f6/liuxiao/BlockLM-ssm/config/config_block_10B.json']' returned non-zero exit status 1.
