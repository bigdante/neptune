[2021-12-12 11:17:50,515] [WARNING] [runner.py:122:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2021-12-12 11:17:51,534] [INFO] [runner.py:360:main] cmd = /opt/conda/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=50436 pretrain_glm.py --block-lm --task-mask --bert-prob 0.5 --gap-sentence-prob 0.3 --avg-block-length 3 --gpt-min-ratio 0.25 --block-mask-prob 0.1 --short-seq-prob 0.02 --experiment-name blocklm-10b-ssm --model-parallel-size 1 --num-layers 48 --hidden-size 4096 --num-attention-heads 64 --seq-length 513 --max-sequence-length 1025 --save /dataset/fd5061f6/english_data/checkpoints --load /dataset/fd5061f6/english_data/checkpoints/blocklm-10b-512 --old-checkpoint --log-interval 50 --eval-interval 1000 --save-interval 2000 --train-iters 250000 --train-data wikipedia_ssm --resume-dataloader --filter-english --loader-scatter 32 --tokenizer-type GPT2BPETokenizer --split 949,50,1 --distributed-backend nccl --lr-decay-style cosine --lr-decay-ratio 0.1 --lr-decay-iters 175000 --warmup 0.04 --checkpoint-activations --fp16 --deepspeed --deepspeed_config /dataset/fd5061f6/liuxiao/BlockLM-ssm/config/config_block_10B.json
[2021-12-12 11:17:52,159] [INFO] [launch.py:73:main] 0 NCCL_IB_DISABLE 0
[2021-12-12 11:17:52,159] [INFO] [launch.py:73:main] 0 NCCL_DEBUG info
[2021-12-12 11:17:52,159] [INFO] [launch.py:73:main] 0 NCCL_NET_GDR_LEVEL 2
[2021-12-12 11:17:52,159] [INFO] [launch.py:80:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2021-12-12 11:17:52,159] [INFO] [launch.py:86:main] nnodes=1, num_local_procs=8, node_rank=0
[2021-12-12 11:17:52,159] [INFO] [launch.py:101:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2021-12-12 11:17:52,159] [INFO] [launch.py:102:main] dist_world_size=8
[2021-12-12 11:17:52,159] [INFO] [launch.py:104:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
using world size: 8 and model-parallel size: 1 
> initializing model parallel with size 1
[2021-12-12 11:17:56,081] [WARNING] [config.py:79:_sanity_check] DeepSpeedConfig: cpu_offload is deprecated. Please use offload_optimizer.
[2021-12-12 11:17:56,081] [WARNING] [config.py:79:_sanity_check] DeepSpeedConfig: cpu_offload is deprecated. Please use offload_optimizer.
[2021-12-12 11:17:56,081] [INFO] [checkpointing.py:734:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
[2021-12-12 11:17:56,082] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
Traceback (most recent call last):
  File "pretrain_glm.py", line 543, in <module>
Traceback (most recent call last):
  File "pretrain_glm.py", line 543, in <module>
        main()main()

  File "pretrain_glm.py", line 450, in main
  File "pretrain_glm.py", line 450, in main
    tokenizer = make_tokenizer(args)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
    tokenizer = make_tokenizer(args)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
[2021-12-12 11:17:56,083] [WARNING] [config.py:79:_sanity_check] DeepSpeedConfig: cpu_offload is deprecated. Please use offload_optimizer.
Traceback (most recent call last):
  File "pretrain_glm.py", line 543, in <module>
    main()
  File "pretrain_glm.py", line 450, in main
    tokenizer = make_tokenizer(args)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
    tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
    tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
[2021-12-12 11:17:56,085] [WARNING] [config.py:79:_sanity_check] DeepSpeedConfig: cpu_offload is deprecated. Please use offload_optimizer.
    tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
[2021-12-12 11:17:56,085] [WARNING] [config.py:79:_sanity_check] DeepSpeedConfig: cpu_offload is deprecated. Please use offload_optimizer.
Traceback (most recent call last):
  File "pretrain_glm.py", line 543, in <module>
[2021-12-12 11:17:56,086] [WARNING] [config.py:79:_sanity_check] DeepSpeedConfig: cpu_offload is deprecated. Please use offload_optimizer.
    Traceback (most recent call last):
main()
  File "pretrain_glm.py", line 450, in main
  File "pretrain_glm.py", line 543, in <module>
    tokenizer = make_tokenizer(args)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
    main()
  File "pretrain_glm.py", line 450, in main
    tokenizer = make_tokenizer(args)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
    get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
    get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
    get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
Traceback (most recent call last):
  File "pretrain_glm.py", line 543, in <module>
    main()
  File "pretrain_glm.py", line 450, in main
    tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
    text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
        text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)
    
text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
    tokenizer = make_tokenizer(args)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
    get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
[2021-12-12 11:17:56,087] [WARNING] [config.py:79:_sanity_check] DeepSpeedConfig: cpu_offload is deprecated. Please use offload_optimizer.
    text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
    get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
    tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
    text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
Traceback (most recent call last):
  File "pretrain_glm.py", line 543, in <module>
[2021-12-12 11:17:56,088] [WARNING] [config.py:79:_sanity_check] DeepSpeedConfig: cpu_offload is deprecated. Please use offload_optimizer.
    main()
  File "pretrain_glm.py", line 450, in main
    tokenizer = make_tokenizer(args)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
            get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)


  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
Traceback (most recent call last):
  File "pretrain_glm.py", line 543, in <module>
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType
    vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)
      File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType
    vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)
  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType
    vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)
  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
    vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)
  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType    
a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType
    main()
  File "pretrain_glm.py", line 450, in main
    tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
    tokenizer = make_tokenizer(args)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
    tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
    get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
    text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
    get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
    text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
    vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)
  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
    vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)
  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType
Killing subprocess 346706
Killing subprocess 346707
Killing subprocess 346708
Killing subprocess 346709
Killing subprocess 346710
Killing subprocess 346711
Killing subprocess 346712
Killing subprocess 346713
Traceback (most recent call last):
  File "/opt/conda/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/launcher/launch.py", line 171, in <module>
    main()
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/launcher/launch.py", line 161, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/launcher/launch.py", line 139, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/bin/python', '-u', 'pretrain_glm.py', '--local_rank=7', '--block-lm', '--task-mask', '--bert-prob', '0.5', '--gap-sentence-prob', '0.3', '--avg-block-length', '3', '--gpt-min-ratio', '0.25', '--block-mask-prob', '0.1', '--short-seq-prob', '0.02', '--experiment-name', 'blocklm-10b-ssm', '--model-parallel-size', '1', '--num-layers', '48', '--hidden-size', '4096', '--num-attention-heads', '64', '--seq-length', '513', '--max-sequence-length', '1025', '--save', '/dataset/fd5061f6/english_data/checkpoints', '--load', '/dataset/fd5061f6/english_data/checkpoints/blocklm-10b-512', '--old-checkpoint', '--log-interval', '50', '--eval-interval', '1000', '--save-interval', '2000', '--train-iters', '250000', '--train-data', 'wikipedia_ssm', '--resume-dataloader', '--filter-english', '--loader-scatter', '32', '--tokenizer-type', 'GPT2BPETokenizer', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr-decay-style', 'cosine', '--lr-decay-ratio', '0.1', '--lr-decay-iters', '175000', '--warmup', '0.04', '--checkpoint-activations', '--fp16', '--deepspeed', '--deepspeed_config', '/dataset/fd5061f6/liuxiao/BlockLM-ssm/config/config_block_10B.json']' returned non-zero exit status 1.
