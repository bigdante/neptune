[2021-12-13 04:38:16,849] [WARNING] [runner.py:122:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2021-12-13 04:38:17,879] [INFO] [runner.py:360:main] cmd = /opt/conda/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=13483 pretrain_glm.py --block-lm --task-mask --bert-prob 1.0 --gap-sentence-prob 0.0 --gpt-prob 0.0 --avg-block-length 3 --gpt-min-ratio 0.25 --block-mask-prob 0.1 --short-seq-prob 0.02 --experiment-name blocklm-10b-ssm --model-parallel-size 1 --num-layers 48 --hidden-size 4096 --num-attention-heads 64 --seq-length 513 --max-sequence-length 1025 --save /dataset/fd5061f6/english_data/checkpoints --load /dataset/fd5061f6/english_data/checkpoints/blocklm-10b-512 --old-checkpoint --no-load-optim --log-interval 50 --eval-interval 1000 --save-interval 2000 --train-iters 250000 --train-data wikipedia_ssm --resume-dataloader --filter-english --tokenizer-type GPT2BPETokenizer --tokenizer-model-type gpt2 --split 949,50,1 --distributed-backend nccl --lr-decay-style cosine --lr-decay-ratio 0.1 --lr-decay-iters 175000 --warmup 0.04 --checkpoint-activations --fp16 --deepspeed --deepspeed_config /dataset/fd5061f6/liuxiao/BlockLM-ssm/config/config_block_10B.json
[2021-12-13 04:38:18,536] [INFO] [launch.py:73:main] 0 NCCL_IB_DISABLE 0
[2021-12-13 04:38:18,536] [INFO] [launch.py:73:main] 0 NCCL_DEBUG info
[2021-12-13 04:38:18,536] [INFO] [launch.py:73:main] 0 NCCL_NET_GDR_LEVEL 2
[2021-12-13 04:38:18,536] [INFO] [launch.py:80:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2021-12-13 04:38:18,536] [INFO] [launch.py:86:main] nnodes=1, num_local_procs=8, node_rank=0
[2021-12-13 04:38:18,536] [INFO] [launch.py:101:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2021-12-13 04:38:18,536] [INFO] [launch.py:102:main] dist_world_size=8
[2021-12-13 04:38:18,536] [INFO] [launch.py:104:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
usage: pretrain_glm.py [-h] [--transformer-xl] [--pretrained-bert]
                       [--encoder-decoder]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--hidden-size HIDDEN_SIZE] [--num-layers NUM_LAYERS]
                       [--layernorm-epsilon LAYERNORM_EPSILON]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--max-sequence-length MAX_SEQUENCE_LENGTH]
                       [--vocab-size VOCAB_SIZE]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--sandwich-ln] [--deep-init]
                       [--attention-scale ATTENTION_SCALE]
                       [--experiment-name EXPERIMENT_NAME]
                       [--batch-size BATCH_SIZE] [--weight-decay WEIGHT_DECAY]
                       [--checkpoint-activations]
                       [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
                       [--train-iters TRAIN_ITERS]
                       [--log-interval LOG_INTERVAL]
                       [--exit-interval EXIT_INTERVAL]
                       [--summary-dir SUMMARY_DIR] [--seed SEED]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-style {constant,linear,cosine,exponential}]
                       [--lr-decay-ratio LR_DECAY_RATIO] [--lr LR]
                       [--warmup WARMUP] [--save SAVE] [--load LOAD]
                       [--old-checkpoint] [--save-interval SAVE_INTERVAL]
                       [--mode {pretrain,finetune,inference}]
                       [--resume-dataloader]
                       [--distributed-backend DISTRIBUTED_BACKEND]
                       [--local_rank LOCAL_RANK] [--fp16] [--epochs EPOCHS]
                       [--label-smoothing LABEL_SMOOTHING]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--new-save-directory] [--switch-linear]
                       [--save-epoch SAVE_EPOCH] [--no-save-rng]
                       [--no-load-rng] [--no-save-optim] [--no-load-optim]
                       [--no-load-lr-scheduler] [--no-load-iteration]
                       [--no-deepspeed-load] [--finetune]
                       [--eval-batch-size EVAL_BATCH_SIZE]
                       [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL]
                       [--eval-epoch EVAL_EPOCH]
                       [--eval-seq-length EVAL_SEQ_LENGTH]
                       [--overlapping-eval OVERLAPPING_EVAL]
                       [--temperature TEMPERATURE] [--top_p TOP_P]
                       [--top_k TOP_K] [--num-beams NUM_BEAMS]
                       [--length-penalty LENGTH_PENALTY]
                       [--no-repeat-ngram-size NO_REPEAT_NGRAM_SIZE]
                       [--min-tgt-length MIN_TGT_LENGTH]
                       [--out-seq-length OUT_SEQ_LENGTH]
                       [--input-source INPUT_SOURCE]
                       [--output-path OUTPUT_PATH] [--with-id]
                       [--max-inference-batch-size MAX_INFERENCE_BATCH_SIZE]
                       [--device DEVICE] [--select-topk]
                       [--blank-maskratio BLANK_MASKRATIO]
                       [--model-parallel-size MODEL_PARALLEL_SIZE]
                       [--train-data TRAIN_DATA [TRAIN_DATA ...]]
                       [--valid-data [VALID_DATA [VALID_DATA ...]]]
                       [--test-data [TEST_DATA [TEST_DATA ...]]]
                       [--split SPLIT] [--num-workers NUM_WORKERS]
                       [--data-dir DATA_DIR] [--no-lazy-loader]
                       [--loader-fraction LOADER_FRACTION]
                       [--loader-scatter LOADER_SCATTER] [--loose-json]
                       [--presplit-sentences] [--no-fix-command]
                       [--no-pre-tokenize] [--cache-dir CACHE_DIR]
                       [--seq-length SEQ_LENGTH] [--mem-length MEM_LENGTH]
                       [--non-sentence-start NON_SENTENCE_START]
                       [--sample-one-document] [--load-splits LOAD_SPLITS]
                       [--save-splits SAVE_SPLITS]
                       [--save-test-data SAVE_TEST_DATA]
                       [--multi-task-data [MULTI_TASK_DATA [MULTI_TASK_DATA ...]]]
                       [--multi-task-ratio MULTI_TASK_RATIO]
                       [--multi-seq-length MULTI_SEQ_LENGTH]
                       [--multi-batch-size MULTI_BATCH_SIZE] [--shuffle]
                       [--filter-english]
                       [--dataset-temperature DATASET_TEMPERATURE]
                       [--tokenizer-model-type TOKENIZER_MODEL_TYPE]
                       [--tokenizer-type {CharacterLevelTokenizer,SentencePieceTokenizer,BertWordPieceTokenizer,GPT2BPETokenizer,ChineseSPTokenizer}]
                       [--task TASK] [--load-pretrained LOAD_PRETRAINED]
                       [--pool-token {start,pad,cls}] [--cloze-eval]
                       [--multi-token] [--segment-length SEGMENT_LENGTH]
                       [--loss-func {cross_entropy,hinge,generative,mix}]
                       [--block-lm-ratio BLOCK_LM_RATIO] [--adapet]
                       [--pattern-id PATTERN_ID] [--fast-decode]
                       [--eval-valid] [--validation-metric VALIDATION_METRIC]
                       [--unidirectional] [--src-seq-length SRC_SEQ_LENGTH]
                       [--tgt-seq-length TGT_SEQ_LENGTH]
                       [--adam-beta1 ADAM_BETA1] [--adam-beta2 ADAM_BETA2]
                       [--adam-eps ADAM_EPS] [--optimizer {adam,adafactor}]
                       [--wsc-negative] [--overwrite] [--no-validation]
                       [--lazy-seq2seq-loader] [--continuous-prompt]
                       [--num-prompt-tokens NUM_PROMPT_TOKENS]
                       [--prompt-func {lstm,mlp,none}] [--freeze-transformer]
                       [--tune-prefix-layers TUNE_PREFIX_LAYERS]
                       [--prefix-prompt PREFIX_PROMPT] [--prompt-init]
                       [--block-lm] [--masked-lm] [--bert-prob BERT_PROB]
                       [--gpt-infill-prob GPT_INFILL_PROB]
                       [--gpt-min-ratio GPT_MIN_RATIO]
                       [--gap-sentence-prob GAP_SENTENCE_PROB]
                       [--gap-sentence-ratio GAP_SENTENCE_RATIO]
                       [--avg-block-length AVG_BLOCK_LENGTH]
                       [--short-seq-prob SHORT_SEQ_PROB]
                       [--single-span-prob SINGLE_SPAN_PROB] [--task-mask]
                       [--no-shuffle-block] [--no-block-position]
                       [--sentinel-token] [--block-mask-prob BLOCK_MASK_PROB]
                       [--context-mask-ratio CONTEXT_MASK_RATIO]
                       [--random-position] [--deepspeed]
                       [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
                       [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
usage: pretrain_glm.py [-h] [--transformer-xl] [--pretrained-bert]
                       [--encoder-decoder]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--hidden-size HIDDEN_SIZE] [--num-layers NUM_LAYERS]
                       [--layernorm-epsilon LAYERNORM_EPSILON]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--max-sequence-length MAX_SEQUENCE_LENGTH]
                       [--vocab-size VOCAB_SIZE]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--sandwich-ln] [--deep-init]
                       [--attention-scale ATTENTION_SCALE]
                       [--experiment-name EXPERIMENT_NAME]
                       [--batch-size BATCH_SIZE] [--weight-decay WEIGHT_DECAY]
                       [--checkpoint-activations]
                       [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
                       [--train-iters TRAIN_ITERS]
                       [--log-interval LOG_INTERVAL]
                       [--exit-interval EXIT_INTERVAL]
                       [--summary-dir SUMMARY_DIR] [--seed SEED]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-style {constant,linear,cosine,exponential}]
                       [--lr-decay-ratio LR_DECAY_RATIO] [--lr LR]
                       [--warmup WARMUP] [--save SAVE] [--load LOAD]
                       [--old-checkpoint] [--save-interval SAVE_INTERVAL]
                       [--mode {pretrain,finetune,inference}]
                       [--resume-dataloader]
                       [--distributed-backend DISTRIBUTED_BACKEND]
                       [--local_rank LOCAL_RANK] [--fp16] [--epochs EPOCHS]
                       [--label-smoothing LABEL_SMOOTHING]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--new-save-directory] [--switch-linear]
                       [--save-epoch SAVE_EPOCH] [--no-save-rng]
                       [--no-load-rng] [--no-save-optim] [--no-load-optim]
                       [--no-load-lr-scheduler] [--no-load-iteration]
                       [--no-deepspeed-load] [--finetune]
                       [--eval-batch-size EVAL_BATCH_SIZE]
                       [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL]
                       [--eval-epoch EVAL_EPOCH]
                       [--eval-seq-length EVAL_SEQ_LENGTH]
                       [--overlapping-eval OVERLAPPING_EVAL]
                       [--temperature TEMPERATURE] [--top_p TOP_P]
                       [--top_k TOP_K] [--num-beams NUM_BEAMS]
                       [--length-penalty LENGTH_PENALTY]
                       [--no-repeat-ngram-size NO_REPEAT_NGRAM_SIZE]
                       [--min-tgt-length MIN_TGT_LENGTH]
                       [--out-seq-length OUT_SEQ_LENGTH]
                       [--input-source INPUT_SOURCE]
                       [--output-path OUTPUT_PATH] [--with-id]
                       [--max-inference-batch-size MAX_INFERENCE_BATCH_SIZE]
                       [--device DEVICE] [--select-topk]
                       [--blank-maskratio BLANK_MASKRATIO]
                       [--model-parallel-size MODEL_PARALLEL_SIZE]
                       [--train-data TRAIN_DATA [TRAIN_DATA ...]]
                       [--valid-data [VALID_DATA [VALID_DATA ...]]]
                       [--test-data [TEST_DATA [TEST_DATA ...]]]
                       [--split SPLIT] [--num-workers NUM_WORKERS]
                       [--data-dir DATA_DIR] [--no-lazy-loader]
                       [--loader-fraction LOADER_FRACTION]
                       [--loader-scatter LOADER_SCATTER] [--loose-json]
                       [--presplit-sentences] [--no-fix-command]
                       [--no-pre-tokenize] [--cache-dir CACHE_DIR]
                       [--seq-length SEQ_LENGTH] [--mem-length MEM_LENGTH]
                       [--non-sentence-start NON_SENTENCE_START]
                       [--sample-one-document] [--load-splits LOAD_SPLITS]
                       [--save-splits SAVE_SPLITS]
                       [--save-test-data SAVE_TEST_DATA]
                       [--multi-task-data [MULTI_TASK_DATA [MULTI_TASK_DATA ...]]]
                       [--multi-task-ratio MULTI_TASK_RATIO]
                       [--multi-seq-length MULTI_SEQ_LENGTH]
                       [--multi-batch-size MULTI_BATCH_SIZE] [--shuffle]
                       [--filter-english]
                       [--dataset-temperature DATASET_TEMPERATURE]
                       [--tokenizer-model-type TOKENIZER_MODEL_TYPE]
                       [--tokenizer-type {CharacterLevelTokenizer,SentencePieceTokenizer,BertWordPieceTokenizer,GPT2BPETokenizer,ChineseSPTokenizer}]
                       [--task TASK] [--load-pretrained LOAD_PRETRAINED]
                       [--pool-token {start,pad,cls}] [--cloze-eval]
                       [--multi-token] [--segment-length SEGMENT_LENGTH]
                       [--loss-func {cross_entropy,hinge,generative,mix}]
                       [--block-lm-ratio BLOCK_LM_RATIO] [--adapet]
                       [--pattern-id PATTERN_ID] [--fast-decode]
                       [--eval-valid] [--validation-metric VALIDATION_METRIC]
                       [--unidirectional] [--src-seq-length SRC_SEQ_LENGTH]
                       [--tgt-seq-length TGT_SEQ_LENGTH]
                       [--adam-beta1 ADAM_BETA1] [--adam-beta2 ADAM_BETA2]
                       [--adam-eps ADAM_EPS] [--optimizer {adam,adafactor}]
                       [--wsc-negative] [--overwrite] [--no-validation]
                       [--lazy-seq2seq-loader] [--continuous-prompt]
                       [--num-prompt-tokens NUM_PROMPT_TOKENS]
                       [--prompt-func {lstm,mlp,none}] [--freeze-transformer]
                       [--tune-prefix-layers TUNE_PREFIX_LAYERS]
                       [--prefix-prompt PREFIX_PROMPT] [--prompt-init]
                       [--block-lm] [--masked-lm] [--bert-prob BERT_PROB]
                       [--gpt-infill-prob GPT_INFILL_PROB]
                       [--gpt-min-ratio GPT_MIN_RATIO]
                       [--gap-sentence-prob GAP_SENTENCE_PROB]
                       [--gap-sentence-ratio GAP_SENTENCE_RATIO]
                       [--avg-block-length AVG_BLOCK_LENGTH]
                       [--short-seq-prob SHORT_SEQ_PROB]
                       [--single-span-prob SINGLE_SPAN_PROB] [--task-mask]
                       [--no-shuffle-block] [--no-block-position]
                       [--sentinel-token] [--block-mask-prob BLOCK_MASK_PROB]
                       [--context-mask-ratio CONTEXT_MASK_RATIO]
                       [--random-position] [--deepspeed]
                       [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
                       [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
usage: pretrain_glm.py [-h] [--transformer-xl] [--pretrained-bert]
                       [--encoder-decoder]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--hidden-size HIDDEN_SIZE] [--num-layers NUM_LAYERS]
                       [--layernorm-epsilon LAYERNORM_EPSILON]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--max-sequence-length MAX_SEQUENCE_LENGTH]
                       [--vocab-size VOCAB_SIZE]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--sandwich-ln] [--deep-init]
                       [--attention-scale ATTENTION_SCALE]
                       [--experiment-name EXPERIMENT_NAME]
                       [--batch-size BATCH_SIZE] [--weight-decay WEIGHT_DECAY]
                       [--checkpoint-activations]
                       [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
                       [--train-iters TRAIN_ITERS]
                       [--log-interval LOG_INTERVAL]
                       [--exit-interval EXIT_INTERVAL]
                       [--summary-dir SUMMARY_DIR] [--seed SEED]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-style {constant,linear,cosine,exponential}]
                       [--lr-decay-ratio LR_DECAY_RATIO] [--lr LR]
                       [--warmup WARMUP] [--save SAVE] [--load LOAD]
                       [--old-checkpoint] [--save-interval SAVE_INTERVAL]
                       [--mode {pretrain,finetune,inference}]
                       [--resume-dataloader]
                       [--distributed-backend DISTRIBUTED_BACKEND]
                       [--local_rank LOCAL_RANK] [--fp16] [--epochs EPOCHS]
                       [--label-smoothing LABEL_SMOOTHING]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--new-save-directory] [--switch-linear]
                       [--save-epoch SAVE_EPOCH] [--no-save-rng]
                       [--no-load-rng] [--no-save-optim] [--no-load-optim]
                       [--no-load-lr-scheduler] [--no-load-iteration]
                       [--no-deepspeed-load] [--finetune]
                       [--eval-batch-size EVAL_BATCH_SIZE]
                       [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL]
                       [--eval-epoch EVAL_EPOCH]
                       [--eval-seq-length EVAL_SEQ_LENGTH]
                       [--overlapping-eval OVERLAPPING_EVAL]
                       [--temperature TEMPERATURE] [--top_p TOP_P]
                       [--top_k TOP_K] [--num-beams NUM_BEAMS]
                       [--length-penalty LENGTH_PENALTY]
                       [--no-repeat-ngram-size NO_REPEAT_NGRAM_SIZE]
                       [--min-tgt-length MIN_TGT_LENGTH]
                       [--out-seq-length OUT_SEQ_LENGTH]
                       [--input-source INPUT_SOURCE]
                       [--output-path OUTPUT_PATH] [--with-id]
                       [--max-inference-batch-size MAX_INFERENCE_BATCH_SIZE]
                       [--device DEVICE] [--select-topk]
                       [--blank-maskratio BLANK_MASKRATIO]
                       [--model-parallel-size MODEL_PARALLEL_SIZE]
                       [--train-data TRAIN_DATA [TRAIN_DATA ...]]
                       [--valid-data [VALID_DATA [VALID_DATA ...]]]
                       [--test-data [TEST_DATA [TEST_DATA ...]]]
                       [--split SPLIT] [--num-workers NUM_WORKERS]
                       [--data-dir DATA_DIR] [--no-lazy-loader]
                       [--loader-fraction LOADER_FRACTION]
                       [--loader-scatter LOADER_SCATTER] [--loose-json]
                       [--presplit-sentences] [--no-fix-command]
                       [--no-pre-tokenize] [--cache-dir CACHE_DIR]
                       [--seq-length SEQ_LENGTH] [--mem-length MEM_LENGTH]
                       [--non-sentence-start NON_SENTENCE_START]
                       [--sample-one-document] [--load-splits LOAD_SPLITS]
                       [--save-splits SAVE_SPLITS]
                       [--save-test-data SAVE_TEST_DATA]
                       [--multi-task-data [MULTI_TASK_DATA [MULTI_TASK_DATA ...]]]
                       [--multi-task-ratio MULTI_TASK_RATIO]
                       [--multi-seq-length MULTI_SEQ_LENGTH]
                       [--multi-batch-size MULTI_BATCH_SIZE] [--shuffle]
                       [--filter-english]
                       [--dataset-temperature DATASET_TEMPERATURE]
                       [--tokenizer-model-type TOKENIZER_MODEL_TYPE]
                       [--tokenizer-type {CharacterLevelTokenizer,SentencePieceTokenizer,BertWordPieceTokenizer,GPT2BPETokenizer,ChineseSPTokenizer}]
                       [--task TASK] [--load-pretrained LOAD_PRETRAINED]
                       [--pool-token {start,pad,cls}] [--cloze-eval]
                       [--multi-token] [--segment-length SEGMENT_LENGTH]
                       [--loss-func {cross_entropy,hinge,generative,mix}]
                       [--block-lm-ratio BLOCK_LM_RATIO] [--adapet]
                       [--pattern-id PATTERN_ID] [--fast-decode]
                       [--eval-valid] [--validation-metric VALIDATION_METRIC]
                       [--unidirectional] [--src-seq-length SRC_SEQ_LENGTH]
                       [--tgt-seq-length TGT_SEQ_LENGTH]
                       [--adam-beta1 ADAM_BETA1] [--adam-beta2 ADAM_BETA2]
                       [--adam-eps ADAM_EPS] [--optimizer {adam,adafactor}]
                       [--wsc-negative] [--overwrite] [--no-validation]
                       [--lazy-seq2seq-loader] [--continuous-prompt]
                       [--num-prompt-tokens NUM_PROMPT_TOKENS]
                       [--prompt-func {lstm,mlp,none}] [--freeze-transformer]
                       [--tune-prefix-layers TUNE_PREFIX_LAYERS]
                       [--prefix-prompt PREFIX_PROMPT] [--prompt-init]
                       [--block-lm] [--masked-lm] [--bert-prob BERT_PROB]
                       [--gpt-infill-prob GPT_INFILL_PROB]
                       [--gpt-min-ratio GPT_MIN_RATIO]
                       [--gap-sentence-prob GAP_SENTENCE_PROB]
                       [--gap-sentence-ratio GAP_SENTENCE_RATIO]
                       [--avg-block-length AVG_BLOCK_LENGTH]
                       [--short-seq-prob SHORT_SEQ_PROB]
                       [--single-span-prob SINGLE_SPAN_PROB] [--task-mask]
                       [--no-shuffle-block] [--no-block-position]
                       [--sentinel-token] [--block-mask-prob BLOCK_MASK_PROB]
                       [--context-mask-ratio CONTEXT_MASK_RATIO]
                       [--random-position] [--deepspeed]
                       [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
                       [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
usage: pretrain_glm.py [-h] [--transformer-xl] [--pretrained-bert]
                       [--encoder-decoder]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--hidden-size HIDDEN_SIZE] [--num-layers NUM_LAYERS]
                       [--layernorm-epsilon LAYERNORM_EPSILON]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--max-sequence-length MAX_SEQUENCE_LENGTH]
                       [--vocab-size VOCAB_SIZE]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--sandwich-ln] [--deep-init]
                       [--attention-scale ATTENTION_SCALE]
                       [--experiment-name EXPERIMENT_NAME]
                       [--batch-size BATCH_SIZE] [--weight-decay WEIGHT_DECAY]
                       [--checkpoint-activations]
                       [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
                       [--train-iters TRAIN_ITERS]
                       [--log-interval LOG_INTERVAL]
                       [--exit-interval EXIT_INTERVAL]
                       [--summary-dir SUMMARY_DIR] [--seed SEED]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-style {constant,linear,cosine,exponential}]
                       [--lr-decay-ratio LR_DECAY_RATIO] [--lr LR]
                       [--warmup WARMUP] [--save SAVE] [--load LOAD]
                       [--old-checkpoint] [--save-interval SAVE_INTERVAL]
                       [--mode {pretrain,finetune,inference}]
                       [--resume-dataloader]
                       [--distributed-backend DISTRIBUTED_BACKEND]
                       [--local_rank LOCAL_RANK] [--fp16] [--epochs EPOCHS]
                       [--label-smoothing LABEL_SMOOTHING]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--new-save-directory] [--switch-linear]
                       [--save-epoch SAVE_EPOCH] [--no-save-rng]
                       [--no-load-rng] [--no-save-optim] [--no-load-optim]
                       [--no-load-lr-scheduler] [--no-load-iteration]
                       [--no-deepspeed-load] [--finetune]
                       [--eval-batch-size EVAL_BATCH_SIZE]
                       [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL]
                       [--eval-epoch EVAL_EPOCH]
                       [--eval-seq-length EVAL_SEQ_LENGTH]
                       [--overlapping-eval OVERLAPPING_EVAL]
                       [--temperature TEMPERATURE] [--top_p TOP_P]
                       [--top_k TOP_K] [--num-beams NUM_BEAMS]
                       [--length-penalty LENGTH_PENALTY]
                       [--no-repeat-ngram-size NO_REPEAT_NGRAM_SIZE]
                       [--min-tgt-length MIN_TGT_LENGTH]
                       [--out-seq-length OUT_SEQ_LENGTH]
                       [--input-source INPUT_SOURCE]
                       [--output-path OUTPUT_PATH] [--with-id]
                       [--max-inference-batch-size MAX_INFERENCE_BATCH_SIZE]
                       [--device DEVICE] [--select-topk]
                       [--blank-maskratio BLANK_MASKRATIO]
                       [--model-parallel-size MODEL_PARALLEL_SIZE]
                       [--train-data TRAIN_DATA [TRAIN_DATA ...]]
                       [--valid-data [VALID_DATA [VALID_DATA ...]]]
                       [--test-data [TEST_DATA [TEST_DATA ...]]]
                       [--split SPLIT] [--num-workers NUM_WORKERS]
                       [--data-dir DATA_DIR] [--no-lazy-loader]
                       [--loader-fraction LOADER_FRACTION]
                       [--loader-scatter LOADER_SCATTER] [--loose-json]
                       [--presplit-sentences] [--no-fix-command]
                       [--no-pre-tokenize] [--cache-dir CACHE_DIR]
                       [--seq-length SEQ_LENGTH] [--mem-length MEM_LENGTH]
                       [--non-sentence-start NON_SENTENCE_START]
                       [--sample-one-document] [--load-splits LOAD_SPLITS]
                       [--save-splits SAVE_SPLITS]
                       [--save-test-data SAVE_TEST_DATA]
                       [--multi-task-data [MULTI_TASK_DATA [MULTI_TASK_DATA ...]]]
                       [--multi-task-ratio MULTI_TASK_RATIO]
                       [--multi-seq-length MULTI_SEQ_LENGTH]
                       [--multi-batch-size MULTI_BATCH_SIZE] [--shuffle]
                       [--filter-english]
                       [--dataset-temperature DATASET_TEMPERATURE]
                       [--tokenizer-model-type TOKENIZER_MODEL_TYPE]
                       [--tokenizer-type {CharacterLevelTokenizer,SentencePieceTokenizer,BertWordPieceTokenizer,GPT2BPETokenizer,ChineseSPTokenizer}]
                       [--task TASK] [--load-pretrained LOAD_PRETRAINED]
                       [--pool-token {start,pad,cls}] [--cloze-eval]
                       [--multi-token] [--segment-length SEGMENT_LENGTH]
                       [--loss-func {cross_entropy,hinge,generative,mix}]
                       [--block-lm-ratio BLOCK_LM_RATIO] [--adapet]
                       [--pattern-id PATTERN_ID] [--fast-decode]
                       [--eval-valid] [--validation-metric VALIDATION_METRIC]
                       [--unidirectional] [--src-seq-length SRC_SEQ_LENGTH]
                       [--tgt-seq-length TGT_SEQ_LENGTH]
                       [--adam-beta1 ADAM_BETA1] [--adam-beta2 ADAM_BETA2]
                       [--adam-eps ADAM_EPS] [--optimizer {adam,adafactor}]
                       [--wsc-negative] [--overwrite] [--no-validation]
                       [--lazy-seq2seq-loader] [--continuous-prompt]
                       [--num-prompt-tokens NUM_PROMPT_TOKENS]
                       [--prompt-func {lstm,mlp,none}] [--freeze-transformer]
                       [--tune-prefix-layers TUNE_PREFIX_LAYERS]
                       [--prefix-prompt PREFIX_PROMPT] [--prompt-init]
                       [--block-lm] [--masked-lm] [--bert-prob BERT_PROB]
                       [--gpt-infill-prob GPT_INFILL_PROB]
                       [--gpt-min-ratio GPT_MIN_RATIO]
                       [--gap-sentence-prob GAP_SENTENCE_PROB]
                       [--gap-sentence-ratio GAP_SENTENCE_RATIO]
                       [--avg-block-length AVG_BLOCK_LENGTH]
                       [--short-seq-prob SHORT_SEQ_PROB]
                       [--single-span-prob SINGLE_SPAN_PROB] [--task-mask]
                       [--no-shuffle-block] [--no-block-position]
                       [--sentinel-token] [--block-mask-prob BLOCK_MASK_PROB]
                       [--context-mask-ratio CONTEXT_MASK_RATIO]
                       [--random-position] [--deepspeed]
                       [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
                       [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
usage: pretrain_glm.py [-h] [--transformer-xl] [--pretrained-bert]
                       [--encoder-decoder]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--hidden-size HIDDEN_SIZE] [--num-layers NUM_LAYERS]
                       [--layernorm-epsilon LAYERNORM_EPSILON]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--max-sequence-length MAX_SEQUENCE_LENGTH]
                       [--vocab-size VOCAB_SIZE]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--sandwich-ln] [--deep-init]
                       [--attention-scale ATTENTION_SCALE]
                       [--experiment-name EXPERIMENT_NAME]
                       [--batch-size BATCH_SIZE] [--weight-decay WEIGHT_DECAY]
                       [--checkpoint-activations]
                       [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
                       [--train-iters TRAIN_ITERS]
                       [--log-interval LOG_INTERVAL]
                       [--exit-interval EXIT_INTERVAL]
                       [--summary-dir SUMMARY_DIR] [--seed SEED]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-style {constant,linear,cosine,exponential}]
                       [--lr-decay-ratio LR_DECAY_RATIO] [--lr LR]
                       [--warmup WARMUP] [--save SAVE] [--load LOAD]
                       [--old-checkpoint] [--save-interval SAVE_INTERVAL]
                       [--mode {pretrain,finetune,inference}]
                       [--resume-dataloader]
                       [--distributed-backend DISTRIBUTED_BACKEND]
                       [--local_rank LOCAL_RANK] [--fp16] [--epochs EPOCHS]
                       [--label-smoothing LABEL_SMOOTHING]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--new-save-directory] [--switch-linear]
                       [--save-epoch SAVE_EPOCH] [--no-save-rng]
                       [--no-load-rng] [--no-save-optim] [--no-load-optim]
                       [--no-load-lr-scheduler] [--no-load-iteration]
                       [--no-deepspeed-load] [--finetune]
                       [--eval-batch-size EVAL_BATCH_SIZE]
                       [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL]
                       [--eval-epoch EVAL_EPOCH]
                       [--eval-seq-length EVAL_SEQ_LENGTH]
                       [--overlapping-eval OVERLAPPING_EVAL]
                       [--temperature TEMPERATURE] [--top_p TOP_P]
                       [--top_k TOP_K] [--num-beams NUM_BEAMS]
                       [--length-penalty LENGTH_PENALTY]
                       [--no-repeat-ngram-size NO_REPEAT_NGRAM_SIZE]
                       [--min-tgt-length MIN_TGT_LENGTH]
                       [--out-seq-length OUT_SEQ_LENGTH]
                       [--input-source INPUT_SOURCE]
                       [--output-path OUTPUT_PATH] [--with-id]
                       [--max-inference-batch-size MAX_INFERENCE_BATCH_SIZE]
                       [--device DEVICE] [--select-topk]
                       [--blank-maskratio BLANK_MASKRATIO]
                       [--model-parallel-size MODEL_PARALLEL_SIZE]
                       [--train-data TRAIN_DATA [TRAIN_DATA ...]]
                       [--valid-data [VALID_DATA [VALID_DATA ...]]]
                       [--test-data [TEST_DATA [TEST_DATA ...]]]
                       [--split SPLIT] [--num-workers NUM_WORKERS]
                       [--data-dir DATA_DIR] [--no-lazy-loader]
                       [--loader-fraction LOADER_FRACTION]
                       [--loader-scatter LOADER_SCATTER] [--loose-json]
                       [--presplit-sentences] [--no-fix-command]
                       [--no-pre-tokenize] [--cache-dir CACHE_DIR]
                       [--seq-length SEQ_LENGTH] [--mem-length MEM_LENGTH]
                       [--non-sentence-start NON_SENTENCE_START]
                       [--sample-one-document] [--load-splits LOAD_SPLITS]
                       [--save-splits SAVE_SPLITS]
                       [--save-test-data SAVE_TEST_DATA]
                       [--multi-task-data [MULTI_TASK_DATA [MULTI_TASK_DATA ...]]]
                       [--multi-task-ratio MULTI_TASK_RATIO]
                       [--multi-seq-length MULTI_SEQ_LENGTH]
                       [--multi-batch-size MULTI_BATCH_SIZE] [--shuffle]
                       [--filter-english]
                       [--dataset-temperature DATASET_TEMPERATURE]
                       [--tokenizer-model-type TOKENIZER_MODEL_TYPE]
                       [--tokenizer-type {CharacterLevelTokenizer,SentencePieceTokenizer,BertWordPieceTokenizer,GPT2BPETokenizer,ChineseSPTokenizer}]
                       [--task TASK] [--load-pretrained LOAD_PRETRAINED]
                       [--pool-token {start,pad,cls}] [--cloze-eval]
                       [--multi-token] [--segment-length SEGMENT_LENGTH]
                       [--loss-func {cross_entropy,hinge,generative,mix}]
                       [--block-lm-ratio BLOCK_LM_RATIO] [--adapet]
                       [--pattern-id PATTERN_ID] [--fast-decode]
                       [--eval-valid] [--validation-metric VALIDATION_METRIC]
                       [--unidirectional] [--src-seq-length SRC_SEQ_LENGTH]
                       [--tgt-seq-length TGT_SEQ_LENGTH]
                       [--adam-beta1 ADAM_BETA1] [--adam-beta2 ADAM_BETA2]
                       [--adam-eps ADAM_EPS] [--optimizer {adam,adafactor}]
                       [--wsc-negative] [--overwrite] [--no-validation]
                       [--lazy-seq2seq-loader] [--continuous-prompt]
                       [--num-prompt-tokens NUM_PROMPT_TOKENS]
                       [--prompt-func {lstm,mlp,none}] [--freeze-transformer]
                       [--tune-prefix-layers TUNE_PREFIX_LAYERS]
                       [--prefix-prompt PREFIX_PROMPT] [--prompt-init]
                       [--block-lm] [--masked-lm] [--bert-prob BERT_PROB]
                       [--gpt-infill-prob GPT_INFILL_PROB]
                       [--gpt-min-ratio GPT_MIN_RATIO]
                       [--gap-sentence-prob GAP_SENTENCE_PROB]
                       [--gap-sentence-ratio GAP_SENTENCE_RATIO]
                       [--avg-block-length AVG_BLOCK_LENGTH]
                       [--short-seq-prob SHORT_SEQ_PROB]
                       [--single-span-prob SINGLE_SPAN_PROB] [--task-mask]
                       [--no-shuffle-block] [--no-block-position]
                       [--sentinel-token] [--block-mask-prob BLOCK_MASK_PROB]
                       [--context-mask-ratio CONTEXT_MASK_RATIO]
                       [--random-position] [--deepspeed]
                       [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
                       [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
usage: pretrain_glm.py [-h] [--transformer-xl] [--pretrained-bert]
                       [--encoder-decoder]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--hidden-size HIDDEN_SIZE] [--num-layers NUM_LAYERS]
                       [--layernorm-epsilon LAYERNORM_EPSILON]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--max-sequence-length MAX_SEQUENCE_LENGTH]
                       [--vocab-size VOCAB_SIZE]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--sandwich-ln] [--deep-init]
                       [--attention-scale ATTENTION_SCALE]
                       [--experiment-name EXPERIMENT_NAME]
                       [--batch-size BATCH_SIZE] [--weight-decay WEIGHT_DECAY]
                       [--checkpoint-activations]
                       [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
                       [--train-iters TRAIN_ITERS]
                       [--log-interval LOG_INTERVAL]
                       [--exit-interval EXIT_INTERVAL]
                       [--summary-dir SUMMARY_DIR] [--seed SEED]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-style {constant,linear,cosine,exponential}]
                       [--lr-decay-ratio LR_DECAY_RATIO] [--lr LR]
                       [--warmup WARMUP] [--save SAVE] [--load LOAD]
                       [--old-checkpoint] [--save-interval SAVE_INTERVAL]
                       [--mode {pretrain,finetune,inference}]
                       [--resume-dataloader]
                       [--distributed-backend DISTRIBUTED_BACKEND]
                       [--local_rank LOCAL_RANK] [--fp16] [--epochs EPOCHS]
                       [--label-smoothing LABEL_SMOOTHING]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--new-save-directory] [--switch-linear]
                       [--save-epoch SAVE_EPOCH] [--no-save-rng]
                       [--no-load-rng] [--no-save-optim] [--no-load-optim]
                       [--no-load-lr-scheduler] [--no-load-iteration]
                       [--no-deepspeed-load] [--finetune]
                       [--eval-batch-size EVAL_BATCH_SIZE]
                       [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL]
                       [--eval-epoch EVAL_EPOCH]
                       [--eval-seq-length EVAL_SEQ_LENGTH]
                       [--overlapping-eval OVERLAPPING_EVAL]
                       [--temperature TEMPERATURE] [--top_p TOP_P]
                       [--top_k TOP_K] [--num-beams NUM_BEAMS]
                       [--length-penalty LENGTH_PENALTY]
                       [--no-repeat-ngram-size NO_REPEAT_NGRAM_SIZE]
                       [--min-tgt-length MIN_TGT_LENGTH]
                       [--out-seq-length OUT_SEQ_LENGTH]
                       [--input-source INPUT_SOURCE]
                       [--output-path OUTPUT_PATH] [--with-id]
                       [--max-inference-batch-size MAX_INFERENCE_BATCH_SIZE]
                       [--device DEVICE] [--select-topk]
                       [--blank-maskratio BLANK_MASKRATIO]
                       [--model-parallel-size MODEL_PARALLEL_SIZE]
                       [--train-data TRAIN_DATA [TRAIN_DATA ...]]
                       [--valid-data [VALID_DATA [VALID_DATA ...]]]
                       [--test-data [TEST_DATA [TEST_DATA ...]]]
                       [--split SPLIT] [--num-workers NUM_WORKERS]
                       [--data-dir DATA_DIR] [--no-lazy-loader]
                       [--loader-fraction LOADER_FRACTION]
                       [--loader-scatter LOADER_SCATTER] [--loose-json]
                       [--presplit-sentences] [--no-fix-command]
                       [--no-pre-tokenize] [--cache-dir CACHE_DIR]
                       [--seq-length SEQ_LENGTH] [--mem-length MEM_LENGTH]
                       [--non-sentence-start NON_SENTENCE_START]
                       [--sample-one-document] [--load-splits LOAD_SPLITS]
                       [--save-splits SAVE_SPLITS]
                       [--save-test-data SAVE_TEST_DATA]
                       [--multi-task-data [MULTI_TASK_DATA [MULTI_TASK_DATA ...]]]
                       [--multi-task-ratio MULTI_TASK_RATIO]
                       [--multi-seq-length MULTI_SEQ_LENGTH]
                       [--multi-batch-size MULTI_BATCH_SIZE] [--shuffle]
                       [--filter-english]
                       [--dataset-temperature DATASET_TEMPERATURE]
                       [--tokenizer-model-type TOKENIZER_MODEL_TYPE]
                       [--tokenizer-type {CharacterLevelTokenizer,SentencePieceTokenizer,BertWordPieceTokenizer,GPT2BPETokenizer,ChineseSPTokenizer}]
                       [--task TASK] [--load-pretrained LOAD_PRETRAINED]
                       [--pool-token {start,pad,cls}] [--cloze-eval]
                       [--multi-token] [--segment-length SEGMENT_LENGTH]
                       [--loss-func {cross_entropy,hinge,generative,mix}]
                       [--block-lm-ratio BLOCK_LM_RATIO] [--adapet]
                       [--pattern-id PATTERN_ID] [--fast-decode]
                       [--eval-valid] [--validation-metric VALIDATION_METRIC]
                       [--unidirectional] [--src-seq-length SRC_SEQ_LENGTH]
                       [--tgt-seq-length TGT_SEQ_LENGTH]
                       [--adam-beta1 ADAM_BETA1] [--adam-beta2 ADAM_BETA2]
                       [--adam-eps ADAM_EPS] [--optimizer {adam,adafactor}]
                       [--wsc-negative] [--overwrite] [--no-validation]
                       [--lazy-seq2seq-loader] [--continuous-prompt]
                       [--num-prompt-tokens NUM_PROMPT_TOKENS]
                       [--prompt-func {lstm,mlp,none}] [--freeze-transformer]
                       [--tune-prefix-layers TUNE_PREFIX_LAYERS]
                       [--prefix-prompt PREFIX_PROMPT] [--prompt-init]
                       [--block-lm] [--masked-lm] [--bert-prob BERT_PROB]
                       [--gpt-infill-prob GPT_INFILL_PROB]
                       [--gpt-min-ratio GPT_MIN_RATIO]
                       [--gap-sentence-prob GAP_SENTENCE_PROB]
                       [--gap-sentence-ratio GAP_SENTENCE_RATIO]
                       [--avg-block-length AVG_BLOCK_LENGTH]
                       [--short-seq-prob SHORT_SEQ_PROB]
                       [--single-span-prob SINGLE_SPAN_PROB] [--task-mask]
                       [--no-shuffle-block] [--no-block-position]
                       [--sentinel-token] [--block-mask-prob BLOCK_MASK_PROB]
                       [--context-mask-ratio CONTEXT_MASK_RATIO]
                       [--random-position] [--deepspeed]
                       [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
                       [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
usage: pretrain_glm.py [-h] [--transformer-xl] [--pretrained-bert]
                       [--encoder-decoder]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--hidden-size HIDDEN_SIZE] [--num-layers NUM_LAYERS]
                       [--layernorm-epsilon LAYERNORM_EPSILON]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--max-sequence-length MAX_SEQUENCE_LENGTH]
                       [--vocab-size VOCAB_SIZE]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--sandwich-ln] [--deep-init]
                       [--attention-scale ATTENTION_SCALE]
                       [--experiment-name EXPERIMENT_NAME]
                       [--batch-size BATCH_SIZE] [--weight-decay WEIGHT_DECAY]
                       [--checkpoint-activations]
                       [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
                       [--train-iters TRAIN_ITERS]
                       [--log-interval LOG_INTERVAL]
                       [--exit-interval EXIT_INTERVAL]
                       [--summary-dir SUMMARY_DIR] [--seed SEED]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-style {constant,linear,cosine,exponential}]
                       [--lr-decay-ratio LR_DECAY_RATIO] [--lr LR]
                       [--warmup WARMUP] [--save SAVE] [--load LOAD]
                       [--old-checkpoint] [--save-interval SAVE_INTERVAL]
                       [--mode {pretrain,finetune,inference}]
                       [--resume-dataloader]
                       [--distributed-backend DISTRIBUTED_BACKEND]
                       [--local_rank LOCAL_RANK] [--fp16] [--epochs EPOCHS]
                       [--label-smoothing LABEL_SMOOTHING]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--new-save-directory] [--switch-linear]
                       [--save-epoch SAVE_EPOCH] [--no-save-rng]
                       [--no-load-rng] [--no-save-optim] [--no-load-optim]
                       [--no-load-lr-scheduler] [--no-load-iteration]
                       [--no-deepspeed-load] [--finetune]
                       [--eval-batch-size EVAL_BATCH_SIZE]
                       [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL]
                       [--eval-epoch EVAL_EPOCH]
                       [--eval-seq-length EVAL_SEQ_LENGTH]
                       [--overlapping-eval OVERLAPPING_EVAL]
                       [--temperature TEMPERATURE] [--top_p TOP_P]
                       [--top_k TOP_K] [--num-beams NUM_BEAMS]
                       [--length-penalty LENGTH_PENALTY]
                       [--no-repeat-ngram-size NO_REPEAT_NGRAM_SIZE]
                       [--min-tgt-length MIN_TGT_LENGTH]
                       [--out-seq-length OUT_SEQ_LENGTH]
                       [--input-source INPUT_SOURCE]
                       [--output-path OUTPUT_PATH] [--with-id]
                       [--max-inference-batch-size MAX_INFERENCE_BATCH_SIZE]
                       [--device DEVICE] [--select-topk]
                       [--blank-maskratio BLANK_MASKRATIO]
                       [--model-parallel-size MODEL_PARALLEL_SIZE]
                       [--train-data TRAIN_DATA [TRAIN_DATA ...]]
                       [--valid-data [VALID_DATA [VALID_DATA ...]]]
                       [--test-data [TEST_DATA [TEST_DATA ...]]]
                       [--split SPLIT] [--num-workers NUM_WORKERS]
                       [--data-dir DATA_DIR] [--no-lazy-loader]
                       [--loader-fraction LOADER_FRACTION]
                       [--loader-scatter LOADER_SCATTER] [--loose-json]
                       [--presplit-sentences] [--no-fix-command]
                       [--no-pre-tokenize] [--cache-dir CACHE_DIR]
                       [--seq-length SEQ_LENGTH] [--mem-length MEM_LENGTH]
                       [--non-sentence-start NON_SENTENCE_START]
                       [--sample-one-document] [--load-splits LOAD_SPLITS]
                       [--save-splits SAVE_SPLITS]
                       [--save-test-data SAVE_TEST_DATA]
                       [--multi-task-data [MULTI_TASK_DATA [MULTI_TASK_DATA ...]]]
                       [--multi-task-ratio MULTI_TASK_RATIO]
                       [--multi-seq-length MULTI_SEQ_LENGTH]
                       [--multi-batch-size MULTI_BATCH_SIZE] [--shuffle]
                       [--filter-english]
                       [--dataset-temperature DATASET_TEMPERATURE]
                       [--tokenizer-model-type TOKENIZER_MODEL_TYPE]
                       [--tokenizer-type {CharacterLevelTokenizer,SentencePieceTokenizer,BertWordPieceTokenizer,GPT2BPETokenizer,ChineseSPTokenizer}]
                       [--task TASK] [--load-pretrained LOAD_PRETRAINED]
                       [--pool-token {start,pad,cls}] [--cloze-eval]
                       [--multi-token] [--segment-length SEGMENT_LENGTH]
                       [--loss-func {cross_entropy,hinge,generative,mix}]
                       [--block-lm-ratio BLOCK_LM_RATIO] [--adapet]
                       [--pattern-id PATTERN_ID] [--fast-decode]
                       [--eval-valid] [--validation-metric VALIDATION_METRIC]
                       [--unidirectional] [--src-seq-length SRC_SEQ_LENGTH]
                       [--tgt-seq-length TGT_SEQ_LENGTH]
                       [--adam-beta1 ADAM_BETA1] [--adam-beta2 ADAM_BETA2]
                       [--adam-eps ADAM_EPS] [--optimizer {adam,adafactor}]
                       [--wsc-negative] [--overwrite] [--no-validation]
                       [--lazy-seq2seq-loader] [--continuous-prompt]
                       [--num-prompt-tokens NUM_PROMPT_TOKENS]
                       [--prompt-func {lstm,mlp,none}] [--freeze-transformer]
                       [--tune-prefix-layers TUNE_PREFIX_LAYERS]
                       [--prefix-prompt PREFIX_PROMPT] [--prompt-init]
                       [--block-lm] [--masked-lm] [--bert-prob BERT_PROB]
                       [--gpt-infill-prob GPT_INFILL_PROB]
                       [--gpt-min-ratio GPT_MIN_RATIO]
                       [--gap-sentence-prob GAP_SENTENCE_PROB]
                       [--gap-sentence-ratio GAP_SENTENCE_RATIO]
                       [--avg-block-length AVG_BLOCK_LENGTH]
                       [--short-seq-prob SHORT_SEQ_PROB]
                       [--single-span-prob SINGLE_SPAN_PROB] [--task-mask]
                       [--no-shuffle-block] [--no-block-position]
                       [--sentinel-token] [--block-mask-prob BLOCK_MASK_PROB]
                       [--context-mask-ratio CONTEXT_MASK_RATIO]
                       [--random-position] [--deepspeed]
                       [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
                       [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
pretrain_glm.py: error: unrecognized arguments: --gpt-prob 0.0
pretrain_glm.py: error: unrecognized arguments: --gpt-prob 0.0
pretrain_glm.py: error: unrecognized arguments: --gpt-prob 0.0
pretrain_glm.py: error: unrecognized arguments: --gpt-prob 0.0
pretrain_glm.py: error: unrecognized arguments: --gpt-prob 0.0
pretrain_glm.py: error: unrecognized arguments: --gpt-prob 0.0
pretrain_glm.py: error: unrecognized arguments: --gpt-prob 0.0
usage: pretrain_glm.py [-h] [--transformer-xl] [--pretrained-bert]
                       [--encoder-decoder]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--hidden-size HIDDEN_SIZE] [--num-layers NUM_LAYERS]
                       [--layernorm-epsilon LAYERNORM_EPSILON]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--max-sequence-length MAX_SEQUENCE_LENGTH]
                       [--vocab-size VOCAB_SIZE]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--sandwich-ln] [--deep-init]
                       [--attention-scale ATTENTION_SCALE]
                       [--experiment-name EXPERIMENT_NAME]
                       [--batch-size BATCH_SIZE] [--weight-decay WEIGHT_DECAY]
                       [--checkpoint-activations]
                       [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
                       [--train-iters TRAIN_ITERS]
                       [--log-interval LOG_INTERVAL]
                       [--exit-interval EXIT_INTERVAL]
                       [--summary-dir SUMMARY_DIR] [--seed SEED]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-style {constant,linear,cosine,exponential}]
                       [--lr-decay-ratio LR_DECAY_RATIO] [--lr LR]
                       [--warmup WARMUP] [--save SAVE] [--load LOAD]
                       [--old-checkpoint] [--save-interval SAVE_INTERVAL]
                       [--mode {pretrain,finetune,inference}]
                       [--resume-dataloader]
                       [--distributed-backend DISTRIBUTED_BACKEND]
                       [--local_rank LOCAL_RANK] [--fp16] [--epochs EPOCHS]
                       [--label-smoothing LABEL_SMOOTHING]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--new-save-directory] [--switch-linear]
                       [--save-epoch SAVE_EPOCH] [--no-save-rng]
                       [--no-load-rng] [--no-save-optim] [--no-load-optim]
                       [--no-load-lr-scheduler] [--no-load-iteration]
                       [--no-deepspeed-load] [--finetune]
                       [--eval-batch-size EVAL_BATCH_SIZE]
                       [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL]
                       [--eval-epoch EVAL_EPOCH]
                       [--eval-seq-length EVAL_SEQ_LENGTH]
                       [--overlapping-eval OVERLAPPING_EVAL]
                       [--temperature TEMPERATURE] [--top_p TOP_P]
                       [--top_k TOP_K] [--num-beams NUM_BEAMS]
                       [--length-penalty LENGTH_PENALTY]
                       [--no-repeat-ngram-size NO_REPEAT_NGRAM_SIZE]
                       [--min-tgt-length MIN_TGT_LENGTH]
                       [--out-seq-length OUT_SEQ_LENGTH]
                       [--input-source INPUT_SOURCE]
                       [--output-path OUTPUT_PATH] [--with-id]
                       [--max-inference-batch-size MAX_INFERENCE_BATCH_SIZE]
                       [--device DEVICE] [--select-topk]
                       [--blank-maskratio BLANK_MASKRATIO]
                       [--model-parallel-size MODEL_PARALLEL_SIZE]
                       [--train-data TRAIN_DATA [TRAIN_DATA ...]]
                       [--valid-data [VALID_DATA [VALID_DATA ...]]]
                       [--test-data [TEST_DATA [TEST_DATA ...]]]
                       [--split SPLIT] [--num-workers NUM_WORKERS]
                       [--data-dir DATA_DIR] [--no-lazy-loader]
                       [--loader-fraction LOADER_FRACTION]
                       [--loader-scatter LOADER_SCATTER] [--loose-json]
                       [--presplit-sentences] [--no-fix-command]
                       [--no-pre-tokenize] [--cache-dir CACHE_DIR]
                       [--seq-length SEQ_LENGTH] [--mem-length MEM_LENGTH]
                       [--non-sentence-start NON_SENTENCE_START]
                       [--sample-one-document] [--load-splits LOAD_SPLITS]
                       [--save-splits SAVE_SPLITS]
                       [--save-test-data SAVE_TEST_DATA]
                       [--multi-task-data [MULTI_TASK_DATA [MULTI_TASK_DATA ...]]]
                       [--multi-task-ratio MULTI_TASK_RATIO]
                       [--multi-seq-length MULTI_SEQ_LENGTH]
                       [--multi-batch-size MULTI_BATCH_SIZE] [--shuffle]
                       [--filter-english]
                       [--dataset-temperature DATASET_TEMPERATURE]
                       [--tokenizer-model-type TOKENIZER_MODEL_TYPE]
                       [--tokenizer-type {CharacterLevelTokenizer,SentencePieceTokenizer,BertWordPieceTokenizer,GPT2BPETokenizer,ChineseSPTokenizer}]
                       [--task TASK] [--load-pretrained LOAD_PRETRAINED]
                       [--pool-token {start,pad,cls}] [--cloze-eval]
                       [--multi-token] [--segment-length SEGMENT_LENGTH]
                       [--loss-func {cross_entropy,hinge,generative,mix}]
                       [--block-lm-ratio BLOCK_LM_RATIO] [--adapet]
                       [--pattern-id PATTERN_ID] [--fast-decode]
                       [--eval-valid] [--validation-metric VALIDATION_METRIC]
                       [--unidirectional] [--src-seq-length SRC_SEQ_LENGTH]
                       [--tgt-seq-length TGT_SEQ_LENGTH]
                       [--adam-beta1 ADAM_BETA1] [--adam-beta2 ADAM_BETA2]
                       [--adam-eps ADAM_EPS] [--optimizer {adam,adafactor}]
                       [--wsc-negative] [--overwrite] [--no-validation]
                       [--lazy-seq2seq-loader] [--continuous-prompt]
                       [--num-prompt-tokens NUM_PROMPT_TOKENS]
                       [--prompt-func {lstm,mlp,none}] [--freeze-transformer]
                       [--tune-prefix-layers TUNE_PREFIX_LAYERS]
                       [--prefix-prompt PREFIX_PROMPT] [--prompt-init]
                       [--block-lm] [--masked-lm] [--bert-prob BERT_PROB]
                       [--gpt-infill-prob GPT_INFILL_PROB]
                       [--gpt-min-ratio GPT_MIN_RATIO]
                       [--gap-sentence-prob GAP_SENTENCE_PROB]
                       [--gap-sentence-ratio GAP_SENTENCE_RATIO]
                       [--avg-block-length AVG_BLOCK_LENGTH]
                       [--short-seq-prob SHORT_SEQ_PROB]
                       [--single-span-prob SINGLE_SPAN_PROB] [--task-mask]
                       [--no-shuffle-block] [--no-block-position]
                       [--sentinel-token] [--block-mask-prob BLOCK_MASK_PROB]
                       [--context-mask-ratio CONTEXT_MASK_RATIO]
                       [--random-position] [--deepspeed]
                       [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
                       [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
pretrain_glm.py: error: unrecognized arguments: --gpt-prob 0.0
Killing subprocess 526007
Killing subprocess 526008
Killing subprocess 526009
Killing subprocess 526010
Killing subprocess 526011
Killing subprocess 526012
Killing subprocess 526013
Killing subprocess 526014
Traceback (most recent call last):
  File "/opt/conda/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/launcher/launch.py", line 171, in <module>
    main()
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/launcher/launch.py", line 161, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/launcher/launch.py", line 139, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/bin/python', '-u', 'pretrain_glm.py', '--local_rank=7', '--block-lm', '--task-mask', '--bert-prob', '1.0', '--gap-sentence-prob', '0.0', '--gpt-prob', '0.0', '--avg-block-length', '3', '--gpt-min-ratio', '0.25', '--block-mask-prob', '0.1', '--short-seq-prob', '0.02', '--experiment-name', 'blocklm-10b-ssm', '--model-parallel-size', '1', '--num-layers', '48', '--hidden-size', '4096', '--num-attention-heads', '64', '--seq-length', '513', '--max-sequence-length', '1025', '--save', '/dataset/fd5061f6/english_data/checkpoints', '--load', '/dataset/fd5061f6/english_data/checkpoints/blocklm-10b-512', '--old-checkpoint', '--no-load-optim', '--log-interval', '50', '--eval-interval', '1000', '--save-interval', '2000', '--train-iters', '250000', '--train-data', 'wikipedia_ssm', '--resume-dataloader', '--filter-english', '--tokenizer-type', 'GPT2BPETokenizer', '--tokenizer-model-type', 'gpt2', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr-decay-style', 'cosine', '--lr-decay-ratio', '0.1', '--lr-decay-iters', '175000', '--warmup', '0.04', '--checkpoint-activations', '--fp16', '--deepspeed', '--deepspeed_config', '/dataset/fd5061f6/liuxiao/BlockLM-ssm/config/config_block_10B.json']' returned non-zero exit status 2.
