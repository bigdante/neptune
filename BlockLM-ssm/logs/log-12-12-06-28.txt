[2021-12-12 06:28:20,966] [WARNING] [runner.py:122:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2021-12-12 06:28:22,034] [INFO] [runner.py:360:main] cmd = /opt/conda/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=56971 pretrain_glm.py --block-lm --task-mask --bert-prob 0.5 --gap-sentence-prob 0.3 --avg-block-length 3 --gpt-min-ratio 0.25 --block-mask-prob 0.1 --short-seq-prob 0.02 --experiment-name blocklm-10b-ssm --model-parallel-size 1 --num-layers 48 --hidden-size 4096 --num-attention-heads 64 --seq-length 513 --max-sequence-length 1025 --save /dataset/fd5061f6/english_data/checkpoints --load-pretrained /dataset/fd5061f6/english_data/checkpoints/blocklm-10b-512 --log-interval 50 --eval-interval 1000 --save-interval 2000 --train-iters 250000 --train-data pile cc-news --resume-dataloader --filter-english --loader-scatter 32 --tokenizer-type GPT2BPETokenizer --split 949,50,1 --distributed-backend nccl --lr-decay-style cosine --lr-decay-ratio 0.1 --lr-decay-iters 175000 --warmup 0.04 --checkpoint-activations --fp16 --deepspeed --deepspeed_config /dataset/fd5061f6/liuxiao/BlockLM-ssm/config/config_block_10B.json
[2021-12-12 06:28:22,647] [INFO] [launch.py:73:main] 0 NCCL_IB_DISABLE 0
[2021-12-12 06:28:22,647] [INFO] [launch.py:73:main] 0 NCCL_DEBUG info
[2021-12-12 06:28:22,647] [INFO] [launch.py:73:main] 0 NCCL_NET_GDR_LEVEL 2
[2021-12-12 06:28:22,647] [INFO] [launch.py:80:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2021-12-12 06:28:22,647] [INFO] [launch.py:86:main] nnodes=1, num_local_procs=8, node_rank=0
[2021-12-12 06:28:22,648] [INFO] [launch.py:101:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2021-12-12 06:28:22,648] [INFO] [launch.py:102:main] dist_world_size=8
[2021-12-12 06:28:22,648] [INFO] [launch.py:104:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
using world size: 8 and model-parallel size: 1 
> initializing model parallel with size 1
Traceback (most recent call last):
  File "pretrain_glm.py", line 543, in <module>
Traceback (most recent call last):
  File "pretrain_glm.py", line 543, in <module>
    main()
  File "pretrain_glm.py", line 450, in main
    main()
  File "pretrain_glm.py", line 450, in main
    tokenizer = make_tokenizer(args)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
    tokenizer = make_tokenizer(args)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
[2021-12-12 06:28:25,775] [INFO] [checkpointing.py:734:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
        tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)

[2021-12-12 06:28:25,776] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
        Traceback (most recent call last):
get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)

  File "pretrain_glm.py", line 543, in <module>
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
    main()
  File "pretrain_glm.py", line 450, in main
Traceback (most recent call last):
  File "pretrain_glm.py", line 543, in <module>
    text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,    
text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
    tokenizer = make_tokenizer(args)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
    main()
  File "pretrain_glm.py", line 450, in main
    tokenizer = make_tokenizer(args)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
    tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
    tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
    get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
    vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)
  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
    vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)
  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
    get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
    a = os.fspath(a)
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType
TypeError: expected str, bytes or os.PathLike object, not NoneType
    text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
    text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
        vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)

  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType
Traceback (most recent call last):
  File "pretrain_glm.py", line 543, in <module>
    main()
  File "pretrain_glm.py", line 450, in main
    tokenizer = make_tokenizer(args)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
    tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
    get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
Traceback (most recent call last):
  File "pretrain_glm.py", line 543, in <module>
    text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
    vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)
  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
    main()
  File "pretrain_glm.py", line 450, in main
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType
    tokenizer = make_tokenizer(args)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
    tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
Traceback (most recent call last):
  File "pretrain_glm.py", line 543, in <module>
    get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
    text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
    vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)
  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType
    main()
  File "pretrain_glm.py", line 450, in main
    tokenizer = make_tokenizer(args)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
    tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
    get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
    text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,
Traceback (most recent call last):
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
  File "pretrain_glm.py", line 543, in <module>
    vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)
  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType
    main()
  File "pretrain_glm.py", line 450, in main
    tokenizer = make_tokenizer(args)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
    tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
    get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
    text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
    vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)
  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType
Killing subprocess 329340
Killing subprocess 329341
Killing subprocess 329342
Killing subprocess 329343
Killing subprocess 329344
Killing subprocess 329345
Killing subprocess 329346
Killing subprocess 329347
Traceback (most recent call last):
  File "/opt/conda/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/launcher/launch.py", line 171, in <module>
    main()
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/launcher/launch.py", line 161, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/launcher/launch.py", line 139, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/bin/python', '-u', 'pretrain_glm.py', '--local_rank=7', '--block-lm', '--task-mask', '--bert-prob', '0.5', '--gap-sentence-prob', '0.3', '--avg-block-length', '3', '--gpt-min-ratio', '0.25', '--block-mask-prob', '0.1', '--short-seq-prob', '0.02', '--experiment-name', 'blocklm-10b-ssm', '--model-parallel-size', '1', '--num-layers', '48', '--hidden-size', '4096', '--num-attention-heads', '64', '--seq-length', '513', '--max-sequence-length', '1025', '--save', '/dataset/fd5061f6/english_data/checkpoints', '--load-pretrained', '/dataset/fd5061f6/english_data/checkpoints/blocklm-10b-512', '--log-interval', '50', '--eval-interval', '1000', '--save-interval', '2000', '--train-iters', '250000', '--train-data', 'pile', 'cc-news', '--resume-dataloader', '--filter-english', '--loader-scatter', '32', '--tokenizer-type', 'GPT2BPETokenizer', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr-decay-style', 'cosine', '--lr-decay-ratio', '0.1', '--lr-decay-iters', '175000', '--warmup', '0.04', '--checkpoint-activations', '--fp16', '--deepspeed', '--deepspeed_config', '/dataset/fd5061f6/liuxiao/BlockLM-ssm/config/config_block_10B.json']' returned non-zero exit status 1.
