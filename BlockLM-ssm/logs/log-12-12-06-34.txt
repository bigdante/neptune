[2021-12-12 06:34:53,854] [WARNING] [runner.py:122:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2021-12-12 06:34:54,941] [INFO] [runner.py:360:main] cmd = /opt/conda/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=57265 pretrain_glm.py --block-lm --task-mask --bert-prob 0.5 --gap-sentence-prob 0.3 --avg-block-length 3 --gpt-min-ratio 0.25 --block-mask-prob 0.1 --short-seq-prob 0.02 --experiment-name blocklm-10b-ssm --model-parallel-size 1 --num-layers 48 --hidden-size 4096 --num-attention-heads 64 --seq-length 513 --max-sequence-length 1025 --save /dataset/fd5061f6/english_data/checkpoints --load /dataset/fd5061f6/english_data/checkpoints/blocklm-10b-512 --log-interval 50 --eval-interval 1000 --save-interval 2000 --train-iters 250000 --train-data wikipedia_ssm --resume-dataloader --filter-english --loader-scatter 32 --tokenizer-type GPT2BPETokenizer --split 949,50,1 --distributed-backend nccl --lr-decay-style cosine --lr-decay-ratio 0.1 --lr-decay-iters 175000 --warmup 0.04 --checkpoint-activations --fp16 --deepspeed --deepspeed_config /dataset/fd5061f6/liuxiao/BlockLM-ssm/config/config_block_10B.json
[2021-12-12 06:34:55,576] [INFO] [launch.py:73:main] 0 NCCL_IB_DISABLE 0
[2021-12-12 06:34:55,576] [INFO] [launch.py:73:main] 0 NCCL_DEBUG info
[2021-12-12 06:34:55,576] [INFO] [launch.py:73:main] 0 NCCL_NET_GDR_LEVEL 2
[2021-12-12 06:34:55,576] [INFO] [launch.py:80:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2021-12-12 06:34:55,576] [INFO] [launch.py:86:main] nnodes=1, num_local_procs=8, node_rank=0
[2021-12-12 06:34:55,576] [INFO] [launch.py:101:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2021-12-12 06:34:55,576] [INFO] [launch.py:102:main] dist_world_size=8
[2021-12-12 06:34:55,576] [INFO] [launch.py:104:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
using world size: 8 and model-parallel size: 1 
> initializing model parallel with size 1
[2021-12-12 06:34:59,491] [INFO] [checkpointing.py:734:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
[2021-12-12 06:34:59,491] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
Traceback (most recent call last):
  File "pretrain_glm.py", line 543, in <module>
Traceback (most recent call last):
  File "pretrain_glm.py", line 543, in <module>
Traceback (most recent call last):
  File "pretrain_glm.py", line 543, in <module>
Traceback (most recent call last):
        main()main()  File "pretrain_glm.py", line 543, in <module>


  File "pretrain_glm.py", line 450, in main
  File "pretrain_glm.py", line 450, in main
    main()
    tokenizer = make_tokenizer(args)  File "pretrain_glm.py", line 450, in main

  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
    tokenizer = make_tokenizer(args)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
    main()
  File "pretrain_glm.py", line 450, in main
    tokenizer = make_tokenizer(args)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
Traceback (most recent call last):
    tokenizer = make_tokenizer(args)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
  File "pretrain_glm.py", line 543, in <module>
        tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)

  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
    main()
  File "pretrain_glm.py", line 450, in main
    tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
    tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)
    tokenizer = make_tokenizer(args)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
    tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
        get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)

  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
    get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
        get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)

  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
    text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
        text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,

  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
      File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
    text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
        vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)
    
vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join

  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
        vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)

  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType
Traceback (most recent call last):
  File "pretrain_glm.py", line 543, in <module>
    a = os.fspath(a)
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType
TypeError: expected str, bytes or os.PathLike object, not NoneType
    main()
  File "pretrain_glm.py", line 450, in main
    tokenizer = make_tokenizer(args)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
    tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
    get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
    text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
    vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)
  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType
Traceback (most recent call last):
  File "pretrain_glm.py", line 543, in <module>
    main()
  File "pretrain_glm.py", line 450, in main
    tokenizer = make_tokenizer(args)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
    tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
    get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
    text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
Traceback (most recent call last):
  File "pretrain_glm.py", line 543, in <module>
    vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)
  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType
    main()
  File "pretrain_glm.py", line 450, in main
    tokenizer = make_tokenizer(args)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/configure_data.py", line 42, in make_tokenizer
    tokenizer = get_tokenizer(args, outer_tokenizer=outer_tokenizer)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/__init__.py", line 61, in get_tokenizer
    get_tokenizer.tokenizer = GPT2BPETokenizer(args.tokenizer_model_type, **kwargs)
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization.py", line 367, in __init__
    text_tokenizer = GPT2Tokenizer.from_pretrained(model_type_or_path,
  File "/dataset/fd5061f6/liuxiao/BlockLM-ssm/SwissArmyTransformer/tokenization/glm/tokenization_gpt2.py", line 106, in from_pretrained
    vocab_file = os.path.join(pretrained_model_name_or_path, VOCAB_NAME)
  File "/opt/conda/lib/python3.8/posixpath.py", line 76, in join
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType
Killing subprocess 331017
Killing subprocess 331018
Killing subprocess 331019
Killing subprocess 331020
Killing subprocess 331021
Killing subprocess 331022
Killing subprocess 331023
Killing subprocess 331024
Traceback (most recent call last):
  File "/opt/conda/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/launcher/launch.py", line 171, in <module>
    main()
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/launcher/launch.py", line 161, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/launcher/launch.py", line 139, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/bin/python', '-u', 'pretrain_glm.py', '--local_rank=7', '--block-lm', '--task-mask', '--bert-prob', '0.5', '--gap-sentence-prob', '0.3', '--avg-block-length', '3', '--gpt-min-ratio', '0.25', '--block-mask-prob', '0.1', '--short-seq-prob', '0.02', '--experiment-name', 'blocklm-10b-ssm', '--model-parallel-size', '1', '--num-layers', '48', '--hidden-size', '4096', '--num-attention-heads', '64', '--seq-length', '513', '--max-sequence-length', '1025', '--save', '/dataset/fd5061f6/english_data/checkpoints', '--load', '/dataset/fd5061f6/english_data/checkpoints/blocklm-10b-512', '--log-interval', '50', '--eval-interval', '1000', '--save-interval', '2000', '--train-iters', '250000', '--train-data', 'wikipedia_ssm', '--resume-dataloader', '--filter-english', '--loader-scatter', '32', '--tokenizer-type', 'GPT2BPETokenizer', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr-decay-style', 'cosine', '--lr-decay-ratio', '0.1', '--lr-decay-iters', '175000', '--warmup', '0.04', '--checkpoint-activations', '--fp16', '--deepspeed', '--deepspeed_config', '/dataset/fd5061f6/liuxiao/BlockLM-ssm/config/config_block_10B.json']' returned non-zero exit status 1.
